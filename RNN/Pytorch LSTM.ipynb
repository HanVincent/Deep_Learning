{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/scepter/corpus/written/EFCAMDAT/efcamdat2_tgt.txt'\n",
    "\n",
    "raw_text = open(file, 'r', encoding='utf8').readlines()[:100000]#.replace('\\n', ' ')\n",
    "raw_text = ' '.join(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "batch_size = 32\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.2\n",
    "epochs = 10\n",
    "log_interval = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語料裡所有出現過的 words\n",
    "words = raw_text.split(' ')\n",
    "unq_words = sorted(list(set(words)))\n",
    "\n",
    "# 給每個 word 一個對應的 index，比較好做接下來的任務\n",
    "word_to_int = dict((w, i) for i, w in enumerate(unq_words))\n",
    "int_to_words = dict((i, w) for i, w in enumerate(unq_words))\n",
    "\n",
    "# 共生成 N 個 input-target pair，\n",
    "# 每個 input 長度為 seq_length，target 長度為 1\n",
    "n_words = len(words)\n",
    "dataX = [] # N x seq_length\n",
    "dataY = [] # N x 1\n",
    "\n",
    "for i in range(0, n_words - seq_length):\n",
    "    seq_in = words[i:i + seq_length]\n",
    "    seq_out = words[i + seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 採用 mini-batch，尾巴不足 batch_size 的直接捨棄\n",
    "n_patterns = len(dataY)\n",
    "n_patterns = n_patterns - n_patterns % batch_size\n",
    "X = dataX[:n_patterns]\n",
    "Y = dataY[:n_patterns]\n",
    "\n",
    "# 把 array 每 batch_size 筆資料包成一組，並包成 tensor\n",
    "X = np.array(X)\n",
    "_, seq_length = X.shape\n",
    "X = X.reshape(-1, batch_size, seq_length)\n",
    "X = torch.cuda.LongTensor(X)\n",
    "\n",
    "Y = np.array(Y)\n",
    "Y = Y.reshape(-1, batch_size)\n",
    "Y = torch.cuda.LongTensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, hidden_dim, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # nn.Embedding 可以幫我們建立好字典中每個字對應的 vector\n",
    "        self.embeddings = nn.Embedding(n_vocab, embedding_dim)\n",
    "        \n",
    "        # LSTM layer，形狀為 (input_size, hidden_size, ...)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout)\n",
    "        \n",
    "        # Fully-connected layer，把 hidden state 線性轉換成 output\n",
    "        self.hidden2out = nn.Linear(hidden_dim, n_vocab)\n",
    "        \n",
    "        \n",
    "    def forward(self, seq_in):\n",
    "        # LSTM 接受的 input 形狀為 (timesteps, batch, features)，\n",
    "        # 即 (seq_length, batch_size, embedding_dim)\n",
    "        # 所以先把形狀為 (batch_size, seq_length) 的 input 轉置後，\n",
    "        # 再把每個 value (char index) 轉成 embedding vector\n",
    "        embeddings = self.embeddings(seq_in.t())\n",
    "        \n",
    "        # LSTM 層的 output (lstm_out) 有每個 timestep 出來的結果\n",
    "        #（也就是每個字進去都會輸出一個 hidden state）\n",
    "        # 這邊我們取最後一層的結果，即最近一次的結果，來預測下一個字\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        ht = lstm_out[-1]\n",
    "        \n",
    "        # 線性轉換至 output\n",
    "        out = self.hidden2out(ht)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, data, log_interval):\n",
    "    # 設一下 flag\n",
    "    model.train()\n",
    "    \n",
    "    # Mini-batch 訓練 \n",
    "    print(len(data))\n",
    "    for batch_i, (seq_in, target) in enumerate(data):\n",
    "        seq_in, target = Variable(seq_in), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq_in)                  # 取得預測\n",
    "        loss = F.cross_entropy(output, target)  # 計算 loss\n",
    "        loss.backward()                         # Backpropagation\n",
    "        optimizer.step()                        # 更新參數\n",
    "        \n",
    "        # Log 訓練進度\n",
    "        if batch_i % log_interval == 0:\n",
    "            print('Train epoch: {} ({:2.0f}%)\\tLoss: {:.6f}'.format(epoch, 100. * batch_i / len(data), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# 載入資料，建立模型\n",
    "train_data = list(zip(X, Y))\n",
    "\n",
    "model = Net(len(unq_words), embedding_dim, hidden_dim, dropout=dropout)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model.load_state_dict(torch.load(f='./rnn/model.ml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "for epoch in range(epochs):\n",
    "    train(model, optimizer, epoch, train_data, log_interval=log_interval)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()      \n",
    "        torch.save(model.state_dict(), 'model.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to other people all live together in a long China .\n",
      " professionals is very bright on weekends .\n",
      " I 've six work in mushrooms .\n",
      " The infections had waving responsibility , he , presence how entered man , he was a black and straight , normal man can called in his sisters .\n",
      " On the other hand , my dreams always 've fainting me in the end of the store opposite the delay without mopped studying Last 1 year ago .\n",
      " Finally , when I opened the door was assigned my business and computer , and submit my responsibilities .\n",
      " I however it later , it was a great flight .\n",
      " I called quickly now .\n",
      " I 'm going to six other Pierre There .\n",
      " I am inviting our friends to You have to come to party on !\n",
      " See you tomorrow .\n",
      " It is opposite the market .\n",
      " So , they affected the trees that food , people days and Rafael you can come to party .\n",
      " I wearing pink sweaters and brown .\n",
      " My sister is fixed and thin .\n",
      " She looks very friendly .\n",
      " My mother 's grandfather , but my little sister , they are wearing Design short black hair and small , brown eyes .\n",
      " My son is rule and the big eyes weather .\n",
      " The ' bowling alley ' 's ' - played backpack and is choosing the such Show ten plastic Find ( & ) ; only cost r ; in 3 , just of 2000 take plus by the frisbee .\n",
      " When I finish their closet is missing system of science the Internet .\n",
      " Nowadays more countries as well as I am result in the company where I study in the university .\n",
      " It is a necessary American knowledge of preparing and start Be maybe form .\n",
      " I never explain the facilities to earthquake and after that I am not the most famous moment !\n",
      " John , We have to get a good life of a mountains , and end began to marry Isabella and Japan .\n",
      " Firstly , some player with the passport , if you signed the airport , you have two more . "
     ]
    }
   ],
   "source": [
    "# 隨機選擇一序列作為開端\n",
    "start = np.random.randint(0, n_patterns - 1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# 共 n_sent 句子要生成\n",
    "cnt = 0\n",
    "while cnt < 1: \n",
    "    seq_in = np.array(pattern)\n",
    "    seq_in = seq_in.reshape(1, -1) # batch_size = 1\n",
    "    seq_in = Variable(torch.cuda.LongTensor(seq_in))\n",
    "    \n",
    "    # 生成此序列下一個字\n",
    "    pred = model(seq_in)\n",
    "    pred = F.softmax(pred).data[0].cpu().numpy() # softmax 後轉成機率分佈\n",
    "    word = np.random.choice(unq_words, p=pred)          # 依機率分佈選字\n",
    "    word_idx = word_to_int[word]\n",
    "    \n",
    "    # 印出\n",
    "    print(word, end=' ')\n",
    "    \n",
    "    # 將字附在原序列後並移除第一個字，作為下一個 input 序列\n",
    "    pattern.append(word_idx)\n",
    "    pattern = pattern[1:]\n",
    "    \n",
    "    # 若印出代表句子結尾的標點符號，則完成一個句子生成\n",
    "    if word == '.':\n",
    "        # restart_seq 決定要不要重新挑選一個序列，或是完成一個完整段落\n",
    "#         if restart_seq:\n",
    "#             start = np.random.randint(0, n_patterns - 1)\n",
    "#             pattern = dataX[start]\n",
    "#             print()\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
